{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will focus on **feature engineering** and **data preprocessing** for the Titanic dataset. Based on the insights from our Exploratory Data Analysis (EDA), we'll clean the data, handle missing values, create new features that might improve model performance, and transform categorical and numerical features into a format suitable for machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the `train.csv` dataset. We'll also make a copy to work with, preserving the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('../data/train.csv')\n",
    "    # Create a copy to work with, so the original DataFrame remains untouched\n",
    "    data = df.copy()\n",
    "    print(\"Dataset loaded successfully and a working copy created.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: train.csv not found. Please make sure it's in the 'data/' directory.\")\n",
    "    data = pd.DataFrame() # Create an empty DataFrame to avoid errors later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identified `Age`, `Embarked`, and `Cabin` as columns with missing values in our EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Impute `Age`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll impute missing `Age` values with the **median** age. The median is often preferred over the mean for skewed distributions or when outliers are present, as it's more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Impute Age with median\n",
    "    median_age = data['Age'].median()\n",
    "    data['Age'].fillna(median_age, inplace=True)\n",
    "    print(f\"Missing 'Age' values imputed with median: {median_age:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Impute `Embarked`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `Embarked`, which has only a couple of missing values, we'll impute with the **mode** (most frequent value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Impute Embarked with mode\n",
    "    mode_embarked = data['Embarked'].mode()[0]\n",
    "    data['Embarked'].fillna(mode_embarked, inplace=True)\n",
    "    print(f\"Missing 'Embarked' values imputed with mode: {mode_embarked}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Handle `Cabin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high percentage of missing values in `Cabin`, we'll simplify it by creating a binary feature: `Has_Cabin`. This feature will indicate whether a passenger had cabin information recorded (1) or not (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Create a new feature 'Has_Cabin'\n",
    "    data['Has_Cabin'] = data['Cabin'].notna().astype(int)\n",
    "    # Drop the original 'Cabin' column\n",
    "    data.drop('Cabin', axis=1, inplace=True)\n",
    "    print(\"Created 'Has_Cabin' feature and dropped original 'Cabin' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Verify Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    print(\"\\nMissing values after imputation and handling:\")\n",
    "    print(data.isnull().sum()[data.isnull().sum() > 0])\n",
    "    if data.isnull().sum().sum() == 0:\n",
    "        print(\"All missing values have been handled.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some new features that might provide more predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. `FamilySize` and `IsAlone`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll combine `SibSp` (siblings/spouses aboard) and `Parch` (parents/children aboard) to create `FamilySize`. From `FamilySize`, we'll derive `IsAlone` to indicate if a passenger traveled alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1 # +1 for the passenger themselves\n",
    "    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)\n",
    "    print(\"Created 'FamilySize' and 'IsAlone' features.\")\n",
    "    display(data[['SibSp', 'Parch', 'FamilySize', 'IsAlone']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. `Title` Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Name` column contains titles (e.g., Mr., Mrs., Miss, Master). These titles often reflect social status and could be indicative of survival. We'll extract them and then group less common titles into a 'Rare' category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "    # Group rare titles\n",
    "    data['Title'] = data['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Mlle', 'Ms', 'Mme'],\n",
    "                                          ['Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Rare', 'Miss', 'Miss', 'Mrs'])\n",
    "    print(\"Extracted and categorized 'Title' feature.\")\n",
    "    print(\"\\nDistribution of Titles:\")\n",
    "    print(data['Title'].value_counts())\n",
    "    data.drop('Name', axis=1, inplace=True) # Drop the original 'Name' column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. `Fare_Bin` (Binning `Fare`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll discretize `Fare` into 4 bins using `qcut` to ensure each bin has roughly the same number of observations. This can help handle its skewed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    data['Fare_Bin'] = pd.qcut(data['Fare'], q=4, labels=['Very_Low', 'Low', 'Medium', 'High'])\n",
    "    print(\"Binned 'Fare' into 'Fare_Bin'.\")\n",
    "    print(\"\\nDistribution of Fare_Bin:\")\n",
    "    print(data['Fare_Bin'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. `Age_Bin` (Binning `Age`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we'll bin `Age` into meaningful categories like 'Child', 'YoungAdult', 'Adult', 'Senior'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Define age bins and labels\n",
    "    age_bins = [0, 12, 25, 60, 100]\n",
    "    age_labels = ['Child', 'YoungAdult', 'Adult', 'Senior']\n",
    "    data['Age_Bin'] = pd.cut(data['Age'], bins=age_bins, labels=age_labels, right=False)\n",
    "    print(\"Binned 'Age' into 'Age_Bin'.\")\n",
    "    print(\"\\nDistribution of Age_Bin:\")\n",
    "    print(data['Age_Bin'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models typically require numerical input. We'll use **One-Hot Encoding** for our categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Identify categorical columns to be one-hot encoded\n",
    "    # Pclass is numerical but treated as categorical due to its discrete nature and influence on survival\n",
    "    categorical_cols = ['Sex', 'Embarked', 'Pclass', 'Title', 'Fare_Bin', 'Age_Bin']\n",
    "\n",
    "    # Create a OneHotEncoder instance\n",
    "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "    # Fit and transform the categorical columns\n",
    "    encoded_features = one_hot_encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "    # Create a DataFrame from the encoded features\n",
    "    encoded_feature_names = one_hot_encoder.get_feature_names_out(categorical_cols)\n",
    "    encoded_df = pd.DataFrame(encoded_features, columns=encoded_feature_names, index=data.index)\n",
    "\n",
    "    # Drop original categorical columns and concatenate with encoded features\n",
    "    data = data.drop(columns=categorical_cols)\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "    print(\"Categorical features successfully One-Hot Encoded.\")\n",
    "    print(\"\\nDataFrame after One-Hot Encoding:\")\n",
    "    display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical features that were not binned (`Age`, `Fare`, `FamilySize`), we'll apply **Standard Scaling**. This transforms the data to have a mean of 0 and a standard deviation of 1, which is important for many machine learning algorithms (e.g., Logistic Regression, SVMs, neural networks) that are sensitive to the scale of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    # Identify numerical columns to be scaled. PassengerId and Survived are excluded.\n",
    "    # Age and Fare are excluded as they have been binned. If not binned, they would be scaled.\n",
    "    numerical_cols_for_scaling = ['Age', 'Fare', 'FamilySize']\n",
    "\n",
    "    # Ensure these columns exist and are numeric before scaling\n",
    "    numerical_cols_for_scaling = [col for col in numerical_cols_for_scaling if col in data.columns and pd.api.types.is_numeric_dtype(data[col])]\n",
    "\n",
    "    if numerical_cols_for_scaling:\n",
    "        # Create a StandardScaler instance\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit and transform the numerical columns\n",
    "        data[numerical_cols_for_scaling] = scaler.fit_transform(data[numerical_cols_for_scaling])\n",
    "        print(\"Numerical features successfully Standard Scaled.\")\n",
    "        print(\"\\nDataFrame after Feature Scaling (scaled columns):\")\n",
    "        display(data[numerical_cols_for_scaling].head())\n",
    "    else:\n",
    "        print(\"No numerical columns selected for scaling or they do not exist.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final DataFrame Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the final structure of our preprocessed DataFrame. We should see all missing values handled, new features created, and all relevant features encoded and scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    print(\"\\nFinal DataFrame Information:\")\n",
    "    data.info()\n",
    "\n",
    "    print(\"\\nFinal DataFrame Head:\")\n",
    "    display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should drop `PassengerId` and `Ticket` as they are identifiers and typically do not contribute to predictive power. Also drop the original `Age` and `Fare` columns since we created binned versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    columns_to_drop = ['PassengerId', 'Ticket', 'Age', 'Fare']\n",
    "    # Ensure columns exist before dropping\n",
    "    existing_columns_to_drop = [col for col in columns_to_drop if col in data.columns]\n",
    "    data.drop(columns=existing_columns_to_drop, inplace=True)\n",
    "    print(\"Dropped unnecessary columns: \", existing_columns_to_drop)\n",
    "    print(\"\\nFinal DataFrame Head after dropping columns:\")\n",
    "    display(data.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save the Preprocessed DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll save this preprocessed DataFrame to a new CSV file (`processed_data.csv`) in the `data/` directory. This will allow us to easily load the cleaned and engineered data in the next notebook for model training and evaluation, without having to rerun the preprocessing steps every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data.empty:\n",
    "    output_path = '../data/processed_data.csv'\n",
    "    data.to_csv(output_path, index=False)\n",
    "    print(f\"Preprocessed data saved to {output_path}\")\n",
    "else:\n",
    "    print(\"DataFrame is empty, cannot save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
