{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final notebook in our Titanic Survival Prediction project. Here, we will use the preprocessed data to:\n",
    "\n",
    "1.  **Split the data** into training and testing sets.\n",
    "2.  **Select and train** two different classification models: a `LogisticRegression` model and a `RandomForestClassifier`.\n",
    "3.  **Evaluate** the performance of both models using a variety of metrics and visualizations, including Confusion Matrices, Accuracy, Precision, Recall, F1-Score, and ROC curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll load the `processed_data.csv` file, which contains our cleaned, engineered, and scaled features from the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Set a random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "try:\n",
    "    processed_df = pd.read_csv('../data/processed_data.csv')\n",
    "    print(\"Preprocessed dataset loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: processed_data.csv not found. Please run '02_Feature_Engineering_and_Preprocessing.ipynb' first.\")\n",
    "    processed_df = pd.DataFrame() # Create an empty DataFrame to avoid errors later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's crucial to split our data into training and testing sets. The training set will be used to train our models, and the unseen testing set will be used to evaluate how well our models generalize to new data.\n",
    "\n",
    "We'll use a **Train-Test split (80/20)**, and importantly, we'll use `stratify=y` to ensure that the proportion of `Survived` (our target variable) is approximately the same in both training and testing sets. This is vital for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not processed_df.empty:\n",
    "    # Define features (X) and target (y)\n",
    "    X = processed_df.drop('Survived', axis=1)\n",
    "    y = processed_df['Survived']\n",
    "\n",
    "    # Perform Train-Test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "    print(\"Data split into training and testing sets.\")\n",
    "    print(f\"X_train shape: {X_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}\")\n",
    "    print(f\"y_train shape: {y_train.shape}\")\n",
    "    print(f\"y_test shape: {y_test.shape}\")\n",
    "\n",
    "    print(\"\\nSurvival distribution in training set:\")\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print(\"\\nSurvival distribution in test set:\")\n",
    "    print(y_test.value_counts(normalize=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Selection & Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train two different classification models to compare their performance:\n",
    "\n",
    "1.  **Logistic Regression**: A simple yet powerful linear model often used as a baseline.\n",
    "2.  **Random Forest Classifier**: A robust ensemble tree-based model that can capture complex non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    lr_model = LogisticRegression(random_state=RANDOM_STATE, solver='liblinear')\n",
    "    lr_model.fit(X_train, y_train)\n",
    "    print(\"Logistic Regression model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'X_train' in locals() and not X_train.empty:\n",
    "    rf_model = RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    print(\"Random Forest Classifier model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's evaluate how well our trained models perform on the unseen test data. We'll use several key metrics and visualizations to get a comprehensive understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Evaluation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid code repetition, let's define a function to evaluate and print metrics for a given model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Evaluates a trained classification model and prints key metrics.\n",
    "    \"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    print(f\"\\n--- {model_name} Evaluation ---\")\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "                xticklabels=['Did Not Survive', 'Survived'],\n",
    "                yticklabels=['Did Not Survive', 'Survived'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.show()\n",
    "\n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    return fpr, tpr, roc_auc # Return for ROC curve plotting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'lr_model' in locals():\n",
    "    fpr_lr, tpr_lr, auc_lr = evaluate_model(lr_model, X_test, y_test, 'Logistic Regression')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation for Logistic Regression:**\n",
    "-   **Confusion Matrix:**\n",
    "    -   True Positives (TP): Correctly predicted survivors.\n",
    "    -   True Negatives (TN): Correctly predicted non-survivors.\n",
    "    -   False Positives (FP): Predicted survivors, but they did not survive (Type I error).\n",
    "    -   False Negatives (FN): Predicted non-survivors, but they did survive (Type II error).\n",
    "-   **Accuracy:** Overall proportion of correct predictions. Useful when classes are balanced.\n",
    "-   **Precision:** Out of all predicted positives, what fraction were actually positive. Important when the cost of False Positives is high (e.g., predicting someone survived when they didn't).\n",
    "-   **Recall (Sensitivity):** Out of all actual positives, what fraction were correctly predicted positive. Important when the cost of False Negatives is high (e.g., failing to predict someone survived when they did).\n",
    "-   **F1-Score:** The harmonic mean of Precision and Recall. A good balance between the two, especially useful for imbalanced datasets.\n",
    "-   **AUC (Area Under the ROC Curve):** Measures the model's ability to distinguish between positive and negative classes. Higher AUC indicates better performance across all classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Evaluate Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'rf_model' in locals():\n",
    "    fpr_rf, tpr_rf, auc_rf = evaluate_model(rf_model, X_test, y_test, 'Random Forest Classifier')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation for Random Forest Classifier:**\n",
    "Similar to Logistic Regression, evaluate the same metrics. Often, a Random Forest will perform better than a Logistic Regression due to its ability to model complex interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. ROC Curve and AUC Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The Area Under the Curve (AUC) is a summary measure of the ROC curve. A higher AUC indicates a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'fpr_lr' in locals() and 'fpr_rf' in locals():\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr_lr, tpr_lr, color='darkorange', lw=2, label=f'Logistic Regression (AUC = {auc_lr:.2f})')\n",
    "    plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {auc_rf:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Models not trained or evaluated yet. Please run previous cells.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Which Model Performed Better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the metrics obtained for both Logistic Regression and Random Forest Classifier.\n",
    "\n",
    "-   **Accuracy:** Often a good starting point, but can be misleading with imbalanced classes.\n",
    "-   **Precision and Recall:** Depending on the problem, one might be more critical than the other. For Titanic survival, perhaps correctly identifying survivors (high Recall) is important, but not at the cost of too many false alarms (low Precision).\n",
    "-   **F1-Score:** Provides a balanced view of Precision and Recall.\n",
    "-   **AUC:** A robust metric for comparing classifier performance across different thresholds, especially for imbalanced datasets. The model with a higher AUC is generally considered better.\n",
    "\n",
    "Based on the results, you can discuss which model appears to be more suitable for this problem and why. Typically, a more complex model like Random Forest might capture nuances that a linear model like Logistic Regression misses, leading to better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
